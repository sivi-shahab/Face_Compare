# templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: core
  name: {{ .Values.core.name }}
spec:
  replicas: {{ .Values.core.replicas }} # You can adjust the number of replicas as needed
  selector:
    matchLabels:
      app: core
  template:
    metadata:
      labels:
        app: core
    spec:
      containers:
        - env:
            - name: ML_PORT
              value: "{{ .Values.core.port }}"
            - name: IMG_LENGTH_LIMIT
              value: "{{ .Values.max_detect_size }}"
            - name: UWSGI_PROCESSES
              value: "{{ .Values.uwsgi_processes | default "2" }}"
            - name: UWSGI_THREADS
              value: "{{ .Values.uwsgi_threads | default "1" }}"
          image: {{ .Values.core.repository }}:{{ .Values.core.tag }}
          ports:
          - containerPort: {{ .Values.core.port }}
          name: {{ .Values.core.name }}
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: all
                capabilities: [gpu]
      #     resources:
      #       limits:
      #         nvidia.com/gpu: {{ .Values.gpuCount }}
      #       requests:
      #         nvidia.com/gpu: {{ .Values.gpuCount }}
      # nodeSelector:
      #   cloud.google.com/gke-accelerator: nvidia-tesla-t4
      restartPolicy: Always

---
apiVersion: v1
kind: Service
metadata:
  name: compreface-core
spec:
  type: ClusterIP
  selector:
    app: core
  ports:
    - port: {{ .Values.core.port }}

---
# apiVersion: v1
# kind: Pod
# metadata:
#   name: my-gpu-pod
# spec:
#   containers:
#   - name: my-gpu-container
#     image: nvidia/cuda:11.0.3-runtime-ubuntu20.04
#     command: ["/bin/bash", "-c", "--"]
#     args: ["while true; do sleep 600; done;"]
#     resources:
#       limits:
#         nvidia.com/gpu: 2
#   nodeSelector:
#     cloud.google.com/gke-accelerator: nvidia-tesla-t4
